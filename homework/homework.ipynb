{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ca1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2 \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65069fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "#                                  FUNCIONES AUXILIARES\n",
    "# ===============================================================================\n",
    "\n",
    "def save_estimator(estimator, path=\"../files/models/model.pkl.gz\"):\n",
    "    \"\"\"Paso 5: Guarda el modelo comprimido con gzip.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with gzip.open(path, 'wb') as f:\n",
    "        pickle.dump(estimator, f)\n",
    "    \n",
    "def make_grid_search(estimator, param_grid, cv, scoring='balanced_accuracy', verbose=1, n_jobs=-1):\n",
    "    \"\"\"Paso 4: Crea y ejecuta GridSearchCV.\"\"\"\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        verbose=verbose,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    return grid_search\n",
    "\n",
    "def format_cm_for_json(cm, dataset_name):\n",
    "    \"\"\"Paso 7: Formatea la matriz de confusi√≥n seg√∫n el requisito.\"\"\"\n",
    "    return {\n",
    "        'type': 'cm_matrix',\n",
    "        'dataset': dataset_name,\n",
    "     \n",
    "        'true_0': {\"predicted_0\": int(cm[0, 0]), \"predicted_1\": int(cm[0, 1])},\n",
    "        'true_1': {\"predicted_0\": int(cm[1, 0]), \"predicted_1\": int(cm[1, 1])}\n",
    "    }\n",
    "\n",
    "def evaluate_and_save_metrics(model, X_train, y_train, X_test, y_test, output_path):\n",
    "    \"\"\"Paso 6 & 7: Calcula y guarda todas las m√©tricas y CMs en formato JSON por l√≠nea.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # M√©tricas para Train\n",
    "    metrics_train = {\n",
    "        'type': 'metrics', \n",
    "        'dataset': 'train', \n",
    "        'precision': precision_score(y_train, y_train_pred), \n",
    "        'balanced_accuracy': balanced_accuracy_score(y_train, y_train_pred), \n",
    "        'recall': recall_score(y_train, y_train_pred), \n",
    "        'f1_score': f1_score(y_train, y_train_pred)\n",
    "    }\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "    cm_train_dict = format_cm_for_json(cm_train, 'train')\n",
    "\n",
    "    # M√©tricas para Test\n",
    "    metrics_test = {\n",
    "        'type': 'metrics', \n",
    "        'dataset': 'test', \n",
    "        'precision': precision_score(y_test, y_test_pred), \n",
    "        'balanced_accuracy': balanced_accuracy_score(y_test, y_test_pred), \n",
    "        'recall': recall_score(y_test, y_test_pred), \n",
    "        'f1_score': f1_score(y_test, y_test_pred)\n",
    "    }\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "    cm_test_dict = format_cm_for_json(cm_test, 'test')\n",
    "    \n",
    "    data_to_save = [metrics_train, metrics_test, cm_train_dict, cm_test_dict]\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        for item in data_to_save:\n",
    "            f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d574c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "#                                  PASO 1 & 2: CARGA Y LIMPIEZA\n",
    "# ===============================================================================\n",
    "\n",
    "def carga_y_limpieza(train_path, test_path):\n",
    "\n",
    "    train_dataset = pd.read_csv(train_path, compression='zip')\n",
    "    test_dataset = pd.read_csv(test_path, compression='zip')\n",
    "        \n",
    "    # 1.1 Renombrar 'default payment next month' a 'default' y remover 'ID'.\n",
    "    train_dataset.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
    "    test_dataset.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
    "    train_dataset.drop(columns=[\"ID\"], inplace=True)\n",
    "    test_dataset.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "    # 1.2 Agrupar valores > 4 y 0 en EDUCATION en la categor√≠a 'others' (4)\n",
    "    def clean_education(df):\n",
    "        df['EDUCATION'] = np.where(df['EDUCATION'] > 4, 4, df['EDUCATION'])\n",
    "        df['EDUCATION'] = np.where(df['EDUCATION'] == 0, 4, df['EDUCATION'])\n",
    "        return df\n",
    "\n",
    "    train_dataset = clean_education(train_dataset)\n",
    "    test_dataset = clean_education(test_dataset)\n",
    "\n",
    "    # 1.3 Eliminar registros con informaci√≥n no disponible (NaN)\n",
    "    train_dataset.dropna(inplace=True)\n",
    "    test_dataset.dropna(inplace=True)\n",
    "\n",
    "    # PASO 2. Dividir en X e y\n",
    "    X_train = train_dataset.drop(columns=['default'])\n",
    "    y_train = train_dataset['default']\n",
    "    X_test = test_dataset.drop(columns=['default'])\n",
    "    y_test = test_dataset['default']\n",
    "\n",
    "    # Identificar columnas Categ√≥ricas y Num√©ricas\n",
    "\n",
    "    pay_columns = [f\"PAY_{i}\" for i in [0, 2, 3, 4, 5, 6]]\n",
    "    categorical_columns = [\"SEX\", \"EDUCATION\", \"MARRIAGE\"] + pay_columns\n",
    "    \n",
    "    bill_columns = [f\"BILL_AMT{i}\" for i in range(1, 7)]\n",
    "    pay_amt_columns = [f\"PAY_AMT{i}\" for i in range(1, 7)]\n",
    "    numeric_columns = [\"LIMIT_BAL\", \"AGE\"] + bill_columns + pay_amt_columns\n",
    "\n",
    "    # ASEGURAR TIPOS: Forzar tipos para evitar TypeErrors\n",
    "    for col in categorical_columns:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = X_train[col].astype(str) \n",
    "        if col in X_test.columns:\n",
    "            X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "    for col in numeric_columns:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').astype(float)\n",
    "        if col in X_test.columns:\n",
    "            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').astype(float)\n",
    "            \n",
    "    # √öltimo dropna por si la conversi√≥n forzada introdujo NaN\n",
    "    X_train.dropna(inplace=True)\n",
    "    y_train = y_train.loc[X_train.index]\n",
    "    X_test.dropna(inplace=True)\n",
    "    y_test = y_test.loc[X_test.index]\n",
    "    \n",
    "    print(\"‚úÖ Carga y limpieza completadas. Tipos de datos asegurados.\")\n",
    "    \n",
    "    return X_test, X_train, y_test, y_train, categorical_columns, numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395e7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Carga y limpieza completadas. Tipos de datos asegurados.\n",
      "‚úÖ Pipeline creado y robustecido contra TypeErrors.\n",
      "\n",
      "Iniciando b√∫squeda de hiperpar√°metros (GridSearchCV)...\n",
      "Fitting 10 folds for each of 96 candidates, totalling 960 fits\n",
      "\n",
      "‚úÖ Mejor modelo (GridSearch) guardado en 'files/models/model.pkl.gz'\n",
      "\n",
      "‚úÖ M√©tricas y matrices de confusi√≥n guardadas en '../files/output/metrics.json'\n",
      "\n",
      "üéâ Entrenamiento y evaluaci√≥n completados. ¬°El modelo deber√≠a pasar el test!\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "#                                  PASO 3-7: ENTRENAMIENTO Y EVALUACI√ìN\n",
    "# ===============================================================================\n",
    "\n",
    "def train_and_evaluate_logistic_regression():\n",
    "    \n",
    "    train_path = \"../files/input/train_data.csv.zip\"\n",
    "    test_path = \"../files/input/test_data.csv.zip\"\n",
    "    \n",
    "    X_test, X_train, y_test, y_train, categorical_columns, numeric_columns = carga_y_limpieza(train_path, test_path)\n",
    "\n",
    "    # PASO 3: Crear el Pipeline\n",
    "    \n",
    "    # 3.1 Transformadores\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "    \n",
    "    # Transformador Categ√≥rico: Imputer (para NaN) -> OneHotEncoder\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_columns),\n",
    "            ('cat', categorical_transformer, categorical_columns)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    # 3.2 Pipeline Completo\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocesador', preprocessor), \n",
    "        # CR√çTICO: Usar chi2 para seleccionar mejores features despu√©s del OHE\n",
    "        ('selectkbest', SelectKBest(score_func=chi2)), \n",
    "        ('estimator', LogisticRegression(solver='liblinear', random_state=42, max_iter=5000))\n",
    "    ])\n",
    "    \n",
    "    print(\"‚úÖ Pipeline creado y robustecido contra TypeErrors.\")\n",
    "    \n",
    "    # PASO 4: Optimizaci√≥n de Hiperpar√°metros (GridSearch)\n",
    "    param_grid = {\n",
    "        # Ampliar el rango de K para capturar features importantes\n",
    "        'selectkbest__k': [40, 60, 80, 'all'], \n",
    "        # CR√çTICO: Explorar valores m√°s altos de C (menos regularizaci√≥n)\n",
    "        'estimator__C': [0.1, 1, 10, 50, 200, 500], \n",
    "        'estimator__penalty': ['l1', 'l2'], \n",
    "        'estimator__class_weight': [None, 'balanced']\n",
    "    }\n",
    "\n",
    "    grid_search = make_grid_search(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=10, \n",
    "        scoring='balanced_accuracy', # Usar balanced_accuracy como m√©trica principal\n",
    "        verbose=1,\n",
    "        n_jobs=-1 \n",
    "    )\n",
    "    \n",
    "    print(\"\\nIniciando b√∫squeda de hiperpar√°metros (GridSearchCV)...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # PASO 5: Guardar el mejor modelo\n",
    "    best_model = grid_search\n",
    "    save_estimator(best_model)\n",
    "    print(\"\\n‚úÖ Mejor modelo (GridSearch) guardado en 'files/models/model.pkl.gz'\")\n",
    "\n",
    "    # PASO 6 & 7: Calcular y guardar m√©tricas y matrices de confusi√≥n\n",
    "    output_path = \"../files/output/metrics.json\"\n",
    "    evaluate_and_save_metrics(best_model, X_train, y_train, X_test, y_test, output_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ M√©tricas y matrices de confusi√≥n guardadas en '{output_path}'\")\n",
    "    print(\"\\nüéâ Entrenamiento y evaluaci√≥n completados.\")\n",
    "\n",
    "# Ejecutar el flujo completo\n",
    "if __name__ == '__main__':\n",
    "    train_and_evaluate_logistic_regression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
